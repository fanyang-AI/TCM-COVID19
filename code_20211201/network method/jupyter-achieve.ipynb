{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#datasat\nimport networkx as nx\nimport numpy as np\nfrom torch.utils import data\nfrom torch.utils.data import DataLoader\nimport torch\n\ndef Read_graph(file_name):\n    edge = np.loadtxt(file_name).astype(np.int32)\n    min_node, max_node = edge.min(), edge.max()\n    if min_node == 0:\n        Node = max_node + 1\n    else:\n        Node = max_node\n    # Node = 504\n    G = nx.Graph()\n    Adj = np.zeros([Node, Node], dtype=np.int32)\n    for i in range(edge.shape[0]):\n        G.add_edge(edge[i][0], edge[i][1])\n        if min_node == 0:\n            Adj[edge[i][0], edge[i][1]] = 1\n            Adj[edge[i][1], edge[i][0]] = 1\n        else:\n            Adj[edge[i][0] - 1, edge[i][1] - 1] = 1\n            Adj[edge[i][1] - 1, edge[i][0] - 1] = 1\n    Adj = torch.FloatTensor(Adj)\n    return G, Adj, Node\n\nclass Dataload(data.Dataset):\n\n    def __init__(self, Adj, Node):\n        self.Adj = Adj\n        self.Node = Node\n    def __getitem__(self, index):\n        return index\n        # adj_batch = self.Adj[index]\n        # adj_mat = adj_batch[index]\n        # b_mat = torch.ones_like(adj_batch)\n        # b_mat[adj_batch != 0] = self.Beta\n        # return adj_batch, adj_mat, b_mat\n    def __len__(self):\n        return self.Node","metadata":{"execution":{"iopub.status.busy":"2022-01-01T09:06:19.318722Z","iopub.execute_input":"2022-01-01T09:06:19.31909Z","iopub.status.idle":"2022-01-01T09:06:20.930162Z","shell.execute_reply.started":"2022-01-01T09:06:19.318972Z","shell.execute_reply":"2022-01-01T09:06:20.929087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create coefficient matrix\nimport numpy as np\nimport pandas as pd \n\nmedicine_name_order = pd.read_table(\"../input/yaomingzi/entity2id.txt\",header=None,encoding='gbk',index_col=0)\nmedicine_name_order = list(medicine_name_order.index)[0:480]\n\n#row:medicine  column:compond\nmedi2comp = pd.read_csv(\"../input/womendeai/zhongyao_hhw_matrix_zuizhong_all.csv\",index_col=0)\nmedi2comp = medi2comp.fillna(value=0)\nmedi2comp = medi2comp.loc[medicine_name_order,:]\n#print(medi2comp.shape) (480,12735)\n#row:compond column:protein\ncomp2prot = pd.read_csv(\"../input/womendeai/myda.csv\",index_col=0)\ncomp2prot = comp2prot.fillna(value=0)\ncomp_names = list(medi2comp.columns)\ncomp2prot = comp2prot.loc[comp_names,:]\n#print(comp2prot.shape) (12735,24)\ndef normalized(x):\n    if np.max(x)==0:\n        x == 0\n    else:\n        x -= np.min(x) #为了稳定地计算softmax概率， 一般会减掉最大的那个元素\n        x = x / (np.max(x)-np.min(x))\n    return x\n\n#compond absorb rate\ncomp_caco2 = pd.read_csv(\"../input/hhwcaco2/zhongyao_hhw_CACO2.csv\",encoding='gbk')\ncomp_caco2 = comp_caco2.loc[:,[\"name\",\"MOL_ID\",\"CACO2\"]]\ncomp_caco2 = comp_caco2.fillna(value=0)\ncomp_caco2.head\ncomp_caco2.loc[:,\"CACO2\"] = comp_caco2.loc[:,\"CACO2\"] - min(comp_caco2.loc[:,\"CACO2\"])\ncomp_caco2.loc[:,\"CACO2\"] = comp_caco2.loc[:,\"CACO2\"]/(max(comp_caco2.loc[:,\"CACO2\"])-min(comp_caco2.loc[:,\"CACO2\"]))\n#print(comp_caco2.shape)\n#缺失值归一化后为0.780952\ncomp_caco2 = comp_caco2.drop_duplicates([\"name\",\"MOL_ID\"],keep='first')\n#print(comp_caco2.shape)\nselect_medi = medi2comp.iloc[0]\ncand_comp = comp_caco2[comp_caco2[\"name\"]==select_medi.name]\ncand_comp = cand_comp.sort_values(by='MOL_ID')\n#print(cand_comp)\ncomp_absorb_rate = np.array(cand_comp.loc[:,\"CACO2\"]).reshape(-1,1)\n#comp_absorb_rate\nselect_comp = select_medi[select_medi==1.0].index.tolist()\nmedi_combine_prot = comp2prot.loc[select_comp,]\nmedi_combine_prot = medi_combine_prot.sort_values(by='name')\nmedi_combine_prot = np.array(medi_combine_prot)\nmedi_combine_prot = comp_absorb_rate * medi_combine_prot\nmedi_combine_prot = -np.sum(medi_combine_prot,axis=0).reshape((1,-1))\nmedi_combine_prot = normalized(medi_combine_prot)\n#medi_combine_prot","metadata":{"execution":{"iopub.status.busy":"2022-01-01T09:06:20.932153Z","iopub.execute_input":"2022-01-01T09:06:20.932404Z","iopub.status.idle":"2022-01-01T09:06:24.789061Z","shell.execute_reply.started":"2022-01-01T09:06:20.932374Z","shell.execute_reply":"2022-01-01T09:06:24.788153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(1,medi2comp.shape[0]):\n    #print(i)\n    select_medi = medi2comp.iloc[i]\n    cand_comp = comp_caco2[comp_caco2[\"name\"]==select_medi.name]\n    cand_comp = cand_comp.sort_values(by='MOL_ID')\n    select_comp = select_medi[select_medi==1.0].index.tolist()\n    cand_comp = comp_caco2[comp_caco2[\"name\"]==select_medi.name]\n    if len(select_comp) != 0:\n        candicate = comp2prot.loc[select_comp,]\n        candicate = candicate.sort_values(by='name')\n        if candicate.shape[0] != cand_comp.shape[0]:\n            cand_comp = cand_comp[cand_comp[\"MOL_ID\"].isin(list(candicate.index))]\n            comp_absorb_rate = np.array(cand_comp.loc[:,\"CACO2\"]).reshape(-1,1)\n            candicate = np.array(candicate)\n            candicate = comp_absorb_rate * candicate\n            candicate = -np.sum(candicate,axis=0).reshape((1,-1))\n            candicate = normalized(candicate)\n            medi_combine_prot = np.concatenate((medi_combine_prot,candicate),axis=0)    \n        else:\n            comp_absorb_rate = np.array(cand_comp.loc[:,\"CACO2\"]).reshape(-1,1)\n            candicate = np.array(candicate)\n            candicate = comp_absorb_rate * candicate\n            candicate = -np.sum(candicate,axis=0).reshape((1,-1))\n            candicate = normalized(candicate)\n            medi_combine_prot = np.concatenate((medi_combine_prot,candicate),axis=0)\n    else:\n        candicate = np.zeros(24)\n        medi_combine_prot = np.concatenate((medi_combine_prot,candicate),axis=0)\nprint(medi_combine_prot.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T09:06:24.804179Z","iopub.execute_input":"2022-01-01T09:06:24.804504Z","iopub.status.idle":"2022-01-01T09:06:28.329393Z","shell.execute_reply.started":"2022-01-01T09:06:24.80446Z","shell.execute_reply":"2022-01-01T09:06:28.328405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"medi_combine_prot[:,0] = 20*medi_combine_prot[:,0]\nmedi_combine_prot[:,1:3] = 10*medi_combine_prot[:,1:3]\nmedi_combine_prot[:,3] = 15*medi_combine_prot[:,3]\nprint(medi_combine_prot.shape)\nmedi_combine_prot_T = medi_combine_prot.T\n","metadata":{"execution":{"iopub.status.busy":"2022-01-01T09:06:28.330647Z","iopub.execute_input":"2022-01-01T09:06:28.33123Z","iopub.status.idle":"2022-01-01T09:06:28.338052Z","shell.execute_reply.started":"2022-01-01T09:06:28.331181Z","shell.execute_reply":"2022-01-01T09:06:28.337205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(medi_combine_prot.shape)\n#medi_combine_prot[:,4:] = 1*medi_combine_prot[:,4:]","metadata":{"execution":{"iopub.status.busy":"2022-01-01T09:06:28.339314Z","iopub.execute_input":"2022-01-01T09:06:28.340219Z","iopub.status.idle":"2022-01-01T09:06:28.351938Z","shell.execute_reply.started":"2022-01-01T09:06:28.340173Z","shell.execute_reply":"2022-01-01T09:06:28.351098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#set:medicine-A;compond-B;protein-C\nA2B = np.array(medi2comp)\nA2B_rowsum = np.sum(A2B,axis=1)\nA2B_prob = A2B/A2B_rowsum[:,None]\nA2B_prob = np.nan_to_num(A2B_prob)\n\nB2A = np.transpose(A2B)\nB2A_rowsum = np.sum(B2A,axis=1)\nB2A_prob = B2A/B2A_rowsum[:,None]\nB2A_prob = np.nan_to_num(B2A_prob)\n\nB2C = np.array(comp2prot)\nB2C[B2C>-7] = 0\nB2C[B2C<=-7] = 1\nB2C_rowsum = np.sum(B2C,axis=1)\nB2C_prob = B2C/B2C_rowsum[:,None]\nB2C_prob = np.nan_to_num(B2C_prob)\n\nC2B = np.transpose(B2C)\nC2B_rowsum = np.sum(C2B,axis=1)\nC2B_prob = C2B/C2B_rowsum[:,None]\nC2B_prob = np.nan_to_num(C2B_prob)\n\n#result matrix sets M\nM11 = np.dot(A2B_prob,B2A_prob)\nM22 = np.dot(C2B_prob,B2C_prob)\nM12 = np.dot(A2B_prob,B2C_prob)\nM21 = np.dot(C2B_prob,B2A_prob)\nM1 = np.concatenate((M11,M12),axis=1)\nM2 = np.concatenate((M21,M22),axis=1)\nM = np.concatenate((M1,M2),axis=0)\n#coefficient_matrix create graph\n\nfrom_list = []\nto_list = []\nfor i in range(M.shape[0]):\n    for j in range(M.shape[0]):\n        if i != j :\n            if M[i,j] > 0 :\n                from_list.append(i+1)\n                to_list.append(j+1)\nadj_biaohhh = np.array([from_list,to_list]).T\nadj_biaohhh = adj_biaohhh.astype('int32')\nnp.savetxt(\"./biaobiao.txt\",adj_biaohhh,delimiter=\" \")\n","metadata":{"execution":{"iopub.status.busy":"2022-01-01T09:06:28.353428Z","iopub.execute_input":"2022-01-01T09:06:28.353906Z","iopub.status.idle":"2022-01-01T09:06:29.478283Z","shell.execute_reply.started":"2022-01-01T09:06:28.353853Z","shell.execute_reply":"2022-01-01T09:06:29.477286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#coefficient matrix\naaa = np.concatenate((M11,medi_combine_prot),axis=1)\nbbb = np.concatenate((medi_combine_prot_T,M22),axis=1)\ncoefficient_matrix = np.concatenate((aaa,bbb),axis=0)\n#coefficient_matrix = pd.DataFrame(coefficient_matrix)\n#coefficient_matrix.to_csv(\"./coefficient_matrix.csv\")\ncoefficient_matrix = torch.Tensor(coefficient_matrix)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T09:06:29.479674Z","iopub.execute_input":"2022-01-01T09:06:29.480628Z","iopub.status.idle":"2022-01-01T09:06:29.508079Z","shell.execute_reply.started":"2022-01-01T09:06:29.480575Z","shell.execute_reply":"2022-01-01T09:06:29.507333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"coefficient_matrix.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-01T09:06:29.512413Z","iopub.execute_input":"2022-01-01T09:06:29.513244Z","iopub.status.idle":"2022-01-01T09:06:29.518661Z","shell.execute_reply.started":"2022-01-01T09:06:29.513186Z","shell.execute_reply":"2022-01-01T09:06:29.518004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#models\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.parameter import Parameter\nfrom torch.nn.modules.module import Module\n\nclass MNN(nn.Module):\n    def __init__(self, node_size, nhid0, nhid1, droput, alpha):\n        super(MNN, self).__init__()\n        self.encode0 = nn.Linear(node_size, nhid0)\n        self.encode1 = nn.Linear(nhid0, nhid1)\n        self.decode0 = nn.Linear(nhid1, nhid0)\n        self.decode1 = nn.Linear(nhid0, node_size)\n        self.droput = droput\n        self.alpha = alpha\n\n    def forward(self, adj_batch, adj_mat, b_mat):\n        t0 = F.leaky_relu(self.encode0(adj_batch))\n        t0 = F.leaky_relu(self.encode1(t0))\n        embedding = t0\n        t0 = F.leaky_relu(self.decode0(t0))\n        t0 = F.leaky_relu(self.decode1(t0))\n        embedding_norm = torch.sum(embedding * embedding, dim=1, keepdim=True)\n        L_1st = torch.sum(adj_mat * (embedding_norm -\n                                     2 * torch.mm(embedding, torch.transpose(embedding, dim0=0, dim1=1))\n                                     + torch.transpose(embedding_norm, dim0=0, dim1=1)))\n        L_2nd = torch.sum(((adj_batch - t0) * b_mat) * ((adj_batch - t0) * b_mat))\n        return L_1st, self.alpha * L_2nd, L_1st + self.alpha * L_2nd\n\n    def savector(self, adj):\n        t0 = self.encode0(adj)\n        t0 = self.encode1(t0)\n        return t0","metadata":{"execution":{"iopub.status.busy":"2022-01-01T09:06:29.520603Z","iopub.execute_input":"2022-01-01T09:06:29.521394Z","iopub.status.idle":"2022-01-01T09:06:29.533142Z","shell.execute_reply.started":"2022-01-01T09:06:29.521355Z","shell.execute_reply":"2022-01-01T09:06:29.532492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nimport argparse\nfrom torch.utils.data.dataloader import DataLoader\nimport numpy as np\n\nimport networkx as nx\nimport numpy as np\nfrom torch.utils import data\nfrom torch.utils.data import DataLoader\nimport torch,csv\nimport pandas as pd\nseed = 0\ntorch.manual_seed(seed) # 为CPU设置随机种子\n\ndef parse_args():\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n                            conflict_handler='resolve')\n    parser.add_argument('--input', default='../input/jiuming/biaomatrix.txt',\n                        help='Input graph file')\n    parser.add_argument('--output', default='./vec.txt',\n                        help='Output representation file')\n    parser.add_argument('--workers', default=8, type=int,\n                        help='Number of parallel processes.')\n    parser.add_argument('--weighted', action='store_true', default=False,\n                        help='Treat graph as weighted')\n    parser.add_argument('--epochs', default=1000, type=int,\n                        help='The training epochs of SDNE')\n    parser.add_argument('--dropout', default=0.5, type=float,\n                        help='Dropout rate (1 - keep probability)')\n    parser.add_argument('--weight-decay', type=float, default=5e-4,\n                        help='Weight for L2 loss on embedding matrix')\n    parser.add_argument('--lr', default=0.001, type=float,\n                        help='learning rate')\n    parser.add_argument('--alpha', default=1, type=float,\n                        help='alhpa is a hyperparameter in SDNE')\n    parser.add_argument('--beta', default=5., type=float,\n                        help='beta is a hyperparameter in SDNE')\n    parser.add_argument('--nu1', default=1e-5, type=float,\n                        help='nu1 is a hyperparameter in SDNE')\n    parser.add_argument('--nu2', default=1e-4, type=float,\n                        help='nu2 is a hyperparameter in SDNE')\n    parser.add_argument('--bs', default=100, type=int,\n                        help='batch size of SDNE')\n    parser.add_argument('--nhid0', default=1000, type=int,\n                        help='The first dim')\n    parser.add_argument('--nhid1', default=128, type=int,\n                        help='The second dim')\n    parser.add_argument('--step_size', default=10, type=int,\n                        help='The step size for lr')\n    parser.add_argument('--gamma', default=0.9, type=int,\n                        help='The gamma for lr')\n    args = parser.parse_args(args=[])\n\n    return args","metadata":{"execution":{"iopub.status.busy":"2022-01-01T09:06:29.534736Z","iopub.execute_input":"2022-01-01T09:06:29.535078Z","iopub.status.idle":"2022-01-01T09:06:29.553642Z","shell.execute_reply.started":"2022-01-01T09:06:29.535036Z","shell.execute_reply":"2022-01-01T09:06:29.552952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args = parse_args()\nG, Adj, Node =  Read_graph(args.input)\n# 自身数据集 石膏这个节点与任何药物和蛋白没有任何关联\nmodel = MNN(Node, args.nhid0, args.nhid1, args.dropout, args.alpha)\nopt = optim.Adam(model.parameters(), lr=args.lr)\nscheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=args.step_size, gamma=args.gamma)\nData = Dataload(Adj, Node)\nData = DataLoader(Data, batch_size=args.bs, shuffle=True, )\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T09:06:29.555259Z","iopub.execute_input":"2022-01-01T09:06:29.555807Z","iopub.status.idle":"2022-01-01T09:06:33.48225Z","shell.execute_reply.started":"2022-01-01T09:06:29.55576Z","shell.execute_reply":"2022-01-01T09:06:33.481242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.train()\nfor epoch in range(1, args.epochs + 1):\n    loss_sum, loss_L1, loss_L2, loss_reg = 0, 0, 0, 0\n    for index in Data:\n        #adj_batch = Adj[index]\n        adj_batch = coefficient_matrix[index]\n        #adj_mat = adj_batch[:, index]\n        adj_mat = adj_batch[:, index]\n        #adj_batch = Adj[index]\n        \n        #b_mat = torch.ones_like(adj_batch)\n        b_mat = coefficient_matrix[index]\n        #b_mat[adj_batch != 0] = args.beta\n\n        opt.zero_grad()\n        L_1st, L_2nd, L_all = model(adj_batch, adj_mat, b_mat)\n        L_reg = 0\n        for param in model.parameters():\n            L_reg += args.nu1 * torch.sum(torch.abs(param)) + args.nu2 * torch.sum(param * param)\n        Loss = L_all + L_reg\n        Loss.backward()\n        opt.step()\n        loss_sum += Loss\n        loss_L1 += L_1st\n        loss_L2 += L_2nd\n        loss_reg += L_reg\n    scheduler.step(epoch)\n    # print(\"The lr for epoch %d is %f\" %(epoch, scheduler.get_lr()[0]))\n    print(\"loss for epoch %d is:\" %epoch)\n    print(\"loss_sum is %f\" %loss_sum)\n    #print(\"loss_L1 is %f\" %loss_L1)\n    #print(\"loss_L2 is %f\" %loss_L2)\n    #print(\"loss_reg is %f\" %loss_reg)\nmodel.eval()\nembedding = model.savector(Adj)\noutVec = embedding.detach().numpy()\nnp.savetxt(args.output, outVec)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T09:06:33.483317Z","iopub.execute_input":"2022-01-01T09:06:33.483526Z","iopub.status.idle":"2022-01-01T09:08:15.641768Z","shell.execute_reply.started":"2022-01-01T09:06:33.483501Z","shell.execute_reply":"2022-01-01T09:08:15.6411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport csv\n\n# mapping\nentity2id = {}\nid2entity = {}\nwith open(r\"../input/yaomingzi/entity2id.txt\", newline='', encoding='gbk') as csvfile:\n    reader = csv.DictReader(csvfile, delimiter='\\t', fieldnames=['entity', 'id'])\n    for row_val in reader:\n        id = row_val['id']\n        entity = row_val['entity']\n\n        entity2id[entity] = int(id)\n        id2entity[int(id)] = entity\n\n# print(\"Number of entities: {}\".format(len(entity2id)))\nentity_emb = outVec","metadata":{"execution":{"iopub.status.busy":"2022-01-01T09:08:15.642738Z","iopub.execute_input":"2022-01-01T09:08:15.643277Z","iopub.status.idle":"2022-01-01T09:08:15.657473Z","shell.execute_reply.started":"2022-01-01T09:08:15.643238Z","shell.execute_reply":"2022-01-01T09:08:15.656827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# General Entity Embedding Clustering\n\nfrom matplotlib import cm\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import check_random_state\n\ndataset_id = {}\nwith open( r\"../input/yaomingzi/entity2id.txt\", newline='', encoding='gbk') as csvfile:\n    reader = csv.DictReader(csvfile, delimiter='\\t', fieldnames=['entity','id'])\n    for row_val in reader:\n        id = int(row_val['id'])\n        #print(id)\n\n        if id <= 479:\n            entity_key = \"drug\"\n            if dataset_id.get(entity_key, None) is None:\n                dataset_id[entity_key] = []\n            dataset_id[entity_key].append(row_val['id'])\n        else:\n            entity_key = \"protein\"\n            if dataset_id.get(entity_key, None) is None:\n                dataset_id[entity_key] = []\n            dataset_id[entity_key].append(row_val['id'])\n# Calculate entity cosine similarity\nfrom sklearn.metrics.pairwise import cosine_similarity\nsimilarity = cosine_similarity(entity_emb)\n#print(similarity.shape)\nprotein_id = {}\nfor key in dataset_id[\"protein\"]:\n    protein_key = key\n    if protein_id.get(protein_key, None) is None:\n        protein_id[protein_key] = []\n    protein_id[protein_key].append(id2entity[int(key)])\nprint(protein_id)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T09:08:15.658533Z","iopub.execute_input":"2022-01-01T09:08:15.658862Z","iopub.status.idle":"2022-01-01T09:08:16.583199Z","shell.execute_reply.started":"2022-01-01T09:08:15.658833Z","shell.execute_reply":"2022-01-01T09:08:16.582033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cluster_id = eval(input(\"请输入需要聚类的蛋白的id(可多个，逗号分开,类似这种[480])：\"))\n#print(cluster_id)\ncossim_total = []\nfor i in range(480):\n    for j in cluster_id:\n        cossim = []\n        cossim.append(similarity[i][j])\n        #print(similarity[i][j])\n    cossim_total.append(cossim)\n\ncossim_ave = []\nfor sim in cossim_total:\n    sim_ave = np.mean(sim)\n    cossim_ave.append(sim_ave)\ncossim_ave = np.array(cossim_ave)\ncossim_ave_sort = -(np.sort(-cossim_ave))\nprint(cossim_ave_sort[:10])\ncossim_ave_sort_indx = np.argsort(-cossim_ave)\nprint(cossim_ave_sort_indx[:20])\nfor i in cossim_ave_sort_indx[:20]:\n    print(id2entity[i])","metadata":{"execution":{"iopub.status.busy":"2022-01-01T09:08:16.584758Z","iopub.execute_input":"2022-01-01T09:08:16.586424Z","iopub.status.idle":"2022-01-01T09:11:10.498653Z","shell.execute_reply.started":"2022-01-01T09:08:16.586371Z","shell.execute_reply":"2022-01-01T09:11:10.49737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}